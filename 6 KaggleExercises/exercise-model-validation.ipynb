{"cells":[{"metadata":{},"cell_type":"markdown","source":"**[Introduction to Machine Learning Home Page](https://www.kaggle.com/learn/intro-to-machine-learning)**\n\n---\n"},{"metadata":{},"cell_type":"markdown","source":"## Recap\nYou've built a model. In this exercise you will test how good your model is.\n\nRun the cell below to set up your coding environment where the previous exercise left off."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\ny = home_data.SalePrice\nfeature_columns = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[feature_columns]\n\n# Specify Model\niowa_model = DecisionTreeRegressor()\n# Fit Model\niowa_model.fit(X, y)\n\nprint(\"First in-sample predictions:\", iowa_model.predict(X.head()))\nprint(\"Actual target values for those homes:\", y.head().tolist())\n\n# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.machine_learning.ex4 import *\nprint(\"Setup Complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\n\n## Step 1: Split Your Data\nUse the `train_test_split` function to split up your data.\n\nGive it the argument `random_state=1` so the `check` functions know what to expect when verifying your code.\n\nRecall, your features are loaded in the DataFrame **X** and your target is loaded in **y**.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the train_test_split function and uncomment\nfrom sklearn.model_selection import train_test_split\n\n# fill in and uncomment\ntrain_X, val_X, train_y, val_y = train_test_split(X,y, random_state=1)\n\n# Check your answer\nstep_1.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The lines below will show you a hint or the solution.\nstep_1.hint() \nstep_1.solution()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Specify and Fit the Model\n\nCreate a `DecisionTreeRegressor` model and fit it to the relevant data.\nSet `random_state` to 1 again when creating the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# You imported DecisionTreeRegressor in your last exercise\n# and that code has been copied to the setup code above. So, no need to\n# import it again\n\n# Specify the model\n\niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit iowa_model with the training data\n\niowa_model.fit(train_X,train_y)\n\n# Check your answer\nstep_2.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_2.hint()\nstep_2.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Make Predictions with Validation data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict with all validation observations\nval_predictions = iowa_model.predict(val_X)\n\n# Check your answer\nstep_3.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_3.hint()\nstep_3.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect your predictions and actual values from validation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the top few validation predictions\nprint(val_predictions[0:9])\n      \n# print the top few actual prices from validation data\nprint(val_y[0:9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What do you notice that is different from what you saw with in-sample predictions (which are printed after the top code cell in this page).\n\nDo you remember why validation predictions differ from in-sample (or training) predictions? This is an important idea from the last lesson.\n\n## Step 4: Calculate the Mean Absolute Error in Validation Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nval_mae = mean_absolute_error(val_predictions,val_y)\n\n# uncomment following line to see the validation_mae\nprint(val_mae)\n\n# Check your answer\nstep_4.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_4.hint()\nstep_4.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Additional Markdown to Illustrate MAE across training set and validation set\n\nIn this part of my markdown, I am going to try and demonstrate the MAE concept in training data set and the MAE in test data set. Also, for conceptual understanding of the issue, I am not going to use the predefined function, but I am going to plot the difference between the two data sets. \n\nFor the said plotting, I will be creating a training_difference_array and a test_difference_array. And later on create 2 charts which will evaluate whether a model fit using training dataset performs well on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the plot function using matplotlib\n\nfrom matplotlib.pyplot import plot\n\n#training data plotting\ntraining_difference_vector = train_y - iowa_model.predict(train_X)\ntraining_diff_plot = plot(training_difference_vector, animated = True, c=\"Orange\", ls= \"-.\")\n\ntraining_diff_plot\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing data plotting\n\ntesting_difference_vector = val_y - val_predictions\ntesting_diff_plot = plot(testing_difference_vector, animated = True, c=\"Purple\", ls= \"-.\")\nprint(testing_diff_plot)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A comparison\n\nIn this last section, I am going to just compare the maximum and minimum values of difference from actuals, and in this part, since the direction of difference is not important, we will only consider the absolute values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#absolute value vector creations\n\nabsolute_train_diffs= abs(training_difference_vector)\nabsolute_test_diffs= abs(testing_difference_vector)\n\nmax_abs_train_diff= max(absolute_train_diffs)\nmax_abs_test_diff= max(absolute_test_diffs)\nmin_abs_train_diff= min(absolute_train_diffs)\nmin_abs_test_diff= min(absolute_test_diffs)\n\nprint(absolute_train_diffs.describe())\nprint(plot(absolute_train_diffs))\nprint(\"The maximum value difference takes in training data is \" + str(max_abs_train_diff) +\", and the minimum value it takes in the training data is \"+ str(min_abs_train_diff))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (absolute_test_diffs.describe())\n\nprint(\"The maximum value difference takes in validation data is \" + str(max_abs_test_diff) +\", and the minimum value it takes in the validation data is \"+ str(min_abs_test_diff))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from seaborn import distplot\n\ndistplot(absolute_train_diffs, hist=True, kde=True, color=\"grey\", rug= True, vertical= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distplot(absolute_test_diffs, hist=True, kde=True, color=\"red\", rug= True, vertical= True, axlabel=\"difference\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n**[Introduction to Machine Learning Home Page](https://www.kaggle.com/learn/intro-to-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"},{"metadata":{},"cell_type":"markdown","source":"Is that MAE good?  There isn't a general rule for what values are good that applies across applications. But you'll see how to use (and improve) this number in the next step.\n\n# Keep Going\n\nYou are ready for **[Underfitting and Overfitting](https://www.kaggle.com/dansbecker/underfitting-and-overfitting).**\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}